{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> Предсказание пола клиента </center>\n",
    "\n",
    "### Необходимо выявить пол клиента, основываясь на его транзакционных исторических данных. В роли метрики качества выступает [ROC AUC](https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/), который и нужно будет максимизировать."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Описание файлов\n",
    "- transactions.csv - исторические транзакции банковских клиентов\n",
    "- gender.csv - информация по полу для части клиентов (null - для тестовых)\n",
    "- tr_mcc_codes.csv - mcc-коды транзакций\n",
    "- tr_types.csv - типы транзакций\n",
    "\n",
    "## Описание полей\n",
    "### transactions.csv\n",
    "- customer_id - идентификатор клиента\n",
    "- tr_datetime - день и время совершения транзакции (дни нумеруются с начала данных)\n",
    "- mcc_code - mcc-код транзакции\n",
    "- tr_type - тип транзакции\n",
    "- amount - сумма транзакции в условных единицах; со знаком \"+\" — начисление средств клиенту, \"-\" — списание средств\n",
    "- term_id - идентификатор терминала\n",
    "\n",
    "### gender.csv\n",
    "- customer_id - идентификатор клиента\n",
    "- gender - пол клиента (пустые значения - тестовые клиенты)\n",
    "\n",
    "### tr_mcc_codes.csv\n",
    "- mcc_code - mcc-код транзакции\n",
    "- mcc_description - описание mcc-кода транзакции\n",
    "\n",
    "### tr_types.csv\n",
    "- tr_type - тип транзакции\n",
    "- tr_description - описание типа транзакции"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T13:11:26.066752Z",
     "iopub.status.busy": "2021-08-23T13:11:26.065958Z",
     "iopub.status.idle": "2021-08-23T13:11:26.073084Z",
     "shell.execute_reply": "2021-08-23T13:11:26.072336Z",
     "shell.execute_reply.started": "2021-08-23T13:11:26.066698Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задачи:\n",
    "- Разработать модель бинарной классификации для определения пола клиента. Никаких ограничений к модели - может быть что угодно от KNN до трансформеров. Главное, чтобы ROC AUC на отложенном тесте получился выше 77.5%.\n",
    "- Интерпретировать результаты модели: важность входящих в нее переменных, демонстрация на нескольких примерах, почему получился соответствующий прогноз. Последнее позволит понять, какой пол к какому из таргетов (0/1) принадлежит. Опять же, полная свобода выбора подходов! Полезные ключевые слова: gain, permutation importance, SHAP. \n",
    "- Конвертировать результаты в отчет без кода (идеально - напрямую в [html](https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html))\n",
    "\n",
    "#### P.S. Не забываем про [PEP8](https://www.python.org/dev/peps/pep-0008/)!"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#adding requirements\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tr_mcc_codes = pd.read_csv(\"data/tr_mcc_codes.csv\", sep=\";\", index_col=\"mcc_code\")\n",
    "tr_types = pd.read_csv(\"data/tr_types.csv\", sep=\";\", index_col=\"tr_type\")\n",
    "\n",
    "transactions = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:58.314241Z",
     "start_time": "2019-03-05T17:34:52.056205Z"
    },
    "execution": {
     "iopub.execute_input": "2021-08-23T13:40:21.407638Z",
     "iopub.status.busy": "2021-08-23T13:40:21.407401Z",
     "iopub.status.idle": "2021-08-23T13:40:30.132598Z",
     "shell.execute_reply": "2021-08-23T13:40:30.131875Z",
     "shell.execute_reply.started": "2021-08-23T13:40:21.407616Z"
    },
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot % missing values\n",
    "# (transactions.isnull().sum(axis = 0) / transactions.shape[0] * 100).plot.bar()\n",
    "# plt.title('% Missing')\n",
    "# plt.xlabel('Feature')\n",
    "# plt.ylabel('%')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# #check the classes distribution\n",
    "# import seaborn as sns; sns.set()\n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# x = gender['gender'].value_counts().values\n",
    "# sns.barplot([0,1],x)\n",
    "# plt.title('Target variable count');\n",
    "# print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's check the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# print('>>>>transactions\\n' , transactions.head(10), '\\n')\n",
    "# print('>>>>tr_mcc_codes\\n' , tr_mcc_codes.head(10), '\\n')\n",
    "# print('>>>>tr_types\\n' , tr_types.head(10), '\\n')\n",
    "# print('>>>>gender\\n' , gender.head(10), '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# pca = PCA(2)\n",
    "# df = pca.fit_transform(X)\n",
    " \n",
    "# kmeans = KMeans(n_clusters= 2)\n",
    "# label = kmeans.fit_predict(df)\n",
    "# u_labels = np.unique(label)\n",
    "\n",
    "# # for i in u_labels:\n",
    "# #     plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "# # plt.legend()\n",
    "# # plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# import seaborn as sns; sns.set()\n",
    "# corr = Data.corr()\n",
    "# print(corr)\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "# sns.heatmap(corr);\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let’s start \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def train_feature_engineering(Data, gender):\n",
    "\n",
    "    Data = pd.merge(Data, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "    #feature aggregating\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "    Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "    Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "    Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "    Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "    Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "    Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "    Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "    keys_1 = dict(Data.groupby(Data['mcc_code'])['amount'].count())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_1[x])\n",
    "    Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "    keys_2 = dict(Data.groupby(Data['mcc_code'])['amount'].sum())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_2[x])\n",
    "    Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "    keys_3 = dict(Data.groupby(Data['mcc_code'])['amount'].nunique())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_3[x])\n",
    "    Data['agg_sum_mcc_unqiue_type'] = agg_count_id\n",
    "\n",
    "    mcc_amount_key = dict(Data.set_index(['mcc_code', 'gender']).groupby(level=[0,1])['amount'].mean())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: max(mcc_amount_key[(x, 1.0)], mcc_amount_key[(x, 0.0)]))\n",
    "    Data['mcc_amount_key'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "    Data['days_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "    Data['days_all'] = agg_count_id\n",
    " \n",
    "    keys_mcc = dict(Data.groupby(Data['mcc_code']).apply(lambda x: x['gender'] == 1.0).groupby(level=['mcc_code']).mean())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "    Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "    keys_type = dict(Data.groupby(Data['tr_type']).apply(lambda x: x['gender'] == 1.0).groupby(level=['tr_type']).mean())\n",
    "    agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x])\n",
    "    Data['tr_type_gender'] = agg_count_id\n",
    "\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    pca = PCA(2)\n",
    "    df = pca.fit_transform(Data.dropna)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters= 2)\n",
    "    label = kmeans.fit_predict(df)\n",
    "    u_labels = np.unique(label)\n",
    "\n",
    "    # fill_value = Data['mcc_code'].apply(lambda x: int(keys_mcc[x] > 0.5)) \n",
    "    # Data['gender'].fillna(fill_value, inplace = True)\n",
    "\n",
    "    #print(Data.isnull().sum(axis = 0))\n",
    "\n",
    "    Data.dropna(inplace = True)\n",
    "\n",
    "    return Data, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3 #, my_scaler\n",
    "\n",
    "def test_feature_engineering(Data, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3):\n",
    "\n",
    "    #feature aggregating\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "    Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "    Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "    Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "    Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "    Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "    Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "    Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_1[x])\n",
    "    Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_2[x])\n",
    "    Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_3[x])\n",
    "    Data['agg_sum_mcc_unique_type'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: max(mcc_amount_key[(x, 1.0)], mcc_amount_key[(x, 0.0)]))\n",
    "    Data['mcc_amount_key'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "    Data['days_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "    Data['days_all'] = agg_count_id\n",
    " \n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "    Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x] if x in keys_type else 0)\n",
    "    Data['tr_type_gender'] = agg_count_id\n",
    "    Data.dropna(inplace = True)\n",
    "    \n",
    "    return Data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# def train_feature_engineering(Data, gender):\n",
    "\n",
    "#     Data = pd.merge(Data, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "#     #feature aggregating\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "#     Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "#     Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "#     Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "#     Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "#     Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "#     Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "#     Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].count())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].sum())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].nunique())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_sum_mcc_unqiue_type'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "#     Data['days_unique'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "#     Data['days_all'] = agg_count_id\n",
    " \n",
    "#     keys_mcc = dict(Data.groupby(Data['mcc_code']).apply(lambda x: x['gender'] == 1.0).groupby(level=['mcc_code']).mean())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "#     Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "#     keys_type = dict(Data.groupby(Data['tr_type']).apply(lambda x: x['gender'] == 1.0).groupby(level=['tr_type']).mean())\n",
    "#     agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x])\n",
    "#     Data['tr_type_gender'] = agg_count_id\n",
    "\n",
    "#     fill_value = Data['mcc_code'].apply(lambda x: int(keys_mcc[x] > 0.5)) \n",
    "#     Data['gender'].fillna(fill_value, inplace = True)\n",
    "\n",
    "#     return Data, keys_mcc, keys_type #, my_scaler\n",
    "\n",
    "# def test_feature_engineering(Data, keys_mcc, keys_type):\n",
    "\n",
    "#     #feature aggregating\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "#     Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "#     Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "#     Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "#     Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "#     Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "#     Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "#     Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].count())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].sum())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "#     keys = dict(Data.groupby(Data['mcc_code'])['amount'].nunique())\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys[x])\n",
    "#     Data['agg_sum_mcc_unqiue_type'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "#     Data['days_unique'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "#     Data['days_all'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "#     Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "#     agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x] if x in keys_type else 0)\n",
    "#     Data['tr_type_gender'] = agg_count_id\n",
    "\n",
    " \n",
    "#     Data.dropna(inplace = True)\n",
    "    \n",
    "#     # #scaling\n",
    "#     # indexes = Data.index   \n",
    "#     # columns = Data.columns\n",
    "#     # Data = np.nan_to_num(Data)\n",
    "#     # Data = pd.DataFrame(my_scaler.transform(Data), index=indexes, columns=columns)\n",
    "\n",
    "#     return Data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pca = PCA(2)\n",
    "df = pca.fit_transform(train.dropna)\n",
    "\n",
    "kmeans = KMeans(n_clusters= 2)\n",
    "label = kmeans.fit_predict(df)\n",
    "u_labels = np.unique(label)\n",
    "label"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'method'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-664d85812161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[1;32m    398\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'method'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# df = train.set_index([train.index, 'mcc_code' ,'gender'])\n",
    "# dict(df.groupby(level=[1,2])['amount'].mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_pipeline(drop = True, verbose = False):\n",
    "\n",
    "    split_ratio = 0.99\n",
    "    \n",
    "    tr_mcc_codes = pd.read_csv(\"data/tr_mcc_codes.csv\", sep=\";\", index_col=\"mcc_code\")\n",
    "    tr_types = pd.read_csv(\"data/tr_types.csv\", sep=\";\", index_col=\"tr_type\")\n",
    "\n",
    "    transactions = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "    gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")\n",
    "\n",
    "    transactions.drop(columns=['term_id'], inplace=True)\n",
    "    transactions['tr_datetime'] = transactions['tr_datetime'].apply(lambda x: int(x.split(' ')[0]))\n",
    "\n",
    "    ind = transactions.index.unique()[int(len(transactions.index.unique())*split_ratio)]\n",
    "    train = transactions[transactions.index >= ind]\n",
    "    test = transactions[transactions.index < ind]\n",
    "\n",
    "    print(train.shape, test.shape)\n",
    "\n",
    "    train, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3 = train_feature_engineering(train, gender)\n",
    "    test = test_feature_engineering(test, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3)\n",
    "    \n",
    "    #train = pd.merge(train, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "    test = pd.merge(test, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "    \n",
    "    train = train[train['gender'].notna()]\n",
    "    test = test[test['gender'].notna()]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train, test = preprocess_pipeline(verbose = True)\n",
    "print(train.shape, test.shape )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6739028, 4) (110318, 4)\n",
      "(3663755, 20) (66959, 20)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# x_train.columns, x_test.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# #clustering\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=2)\n",
    "# train[\"Cluster\"] = kmeans.fit_predict(train)\n",
    "# train[\"Cluster\"] = train[\"Cluster\"].astype(\"int64\")\n",
    "\n",
    "# agg_count_id = pd.Series(train['mcc_code'.agg('Cluster')].count(), name = 'mcc_gender', index = train.index)\n",
    "# Data = pd.concat([agg_count_id, train], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# for sex in train.index\n",
    "# train.loc[(1.0, 6011), :]['amount'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# # Data = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "# # gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")\n",
    "# # Data = pd.merge(Data, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "# # group = Data.groupby(Data['mcc_code']).apply(lambda x: x['gender'] == 1.0).groupby(level=['mcc_code']).mean()\n",
    "# # group\n",
    "\n",
    "\n",
    "# #train.set_index(['gender', 'mcc_code'], inplace = True)\n",
    "# train.groupby(train.index).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "y_train = train['gender']\n",
    "x_train = train.drop(columns = ['gender'])\n",
    "\n",
    "y_test = test['gender']\n",
    "x_test = test.drop(columns = ['gender'])\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "len(x_test.index.unique())/(len(x_test.index.unique()) + len(x_train.index.unique())), len(x_test.index.unique())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.011799163179916318, 141)"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "print('>>>>>>>>>', 'XGBClassifier')\n",
    "\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.02, max_delta_step=0, max_depth = 4,\n",
    "              min_child_weight=5, monotone_constraints='()',\n",
    "              n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "              subsample=0.8, tree_method='exact',\n",
    "              validate_parameters=1, verbosity=None)\n",
    "              \n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "y_pred = model.predict(x_train)\n",
    "roc_auc_train = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "print('mean accuracy score on test: ' + str(accuracy))\n",
    "print('roc_auc_score on test: '+ str(roc_auc_test))\n",
    "print('roc_auc_score on train: '+ str(roc_auc_train))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>>>>>>>> XGBClassifier\n",
      "[19:13:14] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "mean accuracy score on test: 0.7044758732955988\n",
      "roc_auc_score on test: 0.6977149793373514\n",
      "roc_auc_score on train: 0.6647218609548196\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('>>>>>>>>>', 'LogisticRegression')\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"mean accuracy score on test: \" + str(lr.score(x_test, y_test)))\n",
    "print(\"mean accuracy score on train: \"+ str(lr.score(x_train, y_train)))\n",
    "\n",
    "y_pred = lr.predict(x_test)\n",
    "print('roc_auc_score on test', roc_auc_score(y_test, y_pred))\n",
    "y_pred = lr.predict(x_train)\n",
    "print('roc_auc_score on train', roc_auc_score(y_train, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>>>>>>>> LogisticRegression\n",
      "mean accuracy score on test: 0.5329081975537269\n",
      "mean accuracy score on train: 0.6610591265460927\n",
      "roc_auc_score on test 0.5497004046752714\n",
      "roc_auc_score on train 0.5293924102792655\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2f42ae9707b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Time taken: %i hours %i minutes and %s seconds.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [5, 6, 7, 8, 9]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "\n",
    "folds = 5\n",
    "param_comb = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(x_train,y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) \n",
    "random_search.fit(x_train, y_train)\n",
    "timer(start_time) "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-acf96f5f99a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n\u001b[0m\u001b[1;32m     10\u001b[0m                     silent=True, nthread=1)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([217.52383785, 244.17107964, 401.51170998, 182.82168097,\n",
      "       218.50906525]), 'std_fit_time': array([ 3.78032963,  4.67087168, 10.44192738,  6.15033259,  7.7312049 ]), 'mean_score_time': array([1.55832853, 1.84535103, 1.89148493, 1.16086679, 1.07255349]), 'std_score_time': array([0.36971757, 0.85082337, 0.36618341, 0.12015103, 0.00763959]), 'param_subsample': masked_array(data=[0.6, 0.8, 0.8, 1.0, 0.6],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[1, 10, 5, 1, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[6, 6, 8, 5, 5],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[5, 1, 1, 0.5, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_colsample_bytree': masked_array(data=[0.6, 0.8, 1.0, 0.8, 0.8],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 6, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 10, 'max_depth': 6, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 8, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 0.5, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 1, 'colsample_bytree': 0.8}], 'split0_test_score': array([0.80069348, 0.79498193, 0.85469537, 0.76685829, 0.76731769]), 'split1_test_score': array([0.80092057, 0.79593103, 0.85415423, 0.7682659 , 0.76894845]), 'split2_test_score': array([0.80316585, 0.79605082, 0.85788501, 0.76501075, 0.76741625]), 'split3_test_score': array([0.80007951, 0.8026417 , 0.85359979, 0.76671059, 0.76493025]), 'split4_test_score': array([0.80242599, 0.79977311, 0.8552406 , 0.76661048, 0.7668639 ]), 'mean_test_score': array([0.80145708, 0.79787571, 0.855115  , 0.7666912 , 0.76709531]), 'std_test_score': array([0.00115129, 0.00289139, 0.00148888, 0.00103329, 0.00129055]), 'rank_test_score': array([2, 3, 1, 5, 4], dtype=int32)}\n",
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.02, max_delta_step=0, max_depth=8,\n",
      "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=True, subsample=0.8, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "\n",
      " Best normalized gini score for 5-fold search with 5 parameter combinations:\n",
      "0.7102299992402887\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 8, 'gamma': 1, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# print('>>>>>>>>>', 'GradientBoostingClassifier')\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=3, random_state=0).fit(x_train, y_train)\n",
    "# print(\"mean accuracy score on test: \" + str(clf.score(x_test, y_test)))\n",
    "# print('roc_auc_score', roc_auc_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>>>>>>>> GradientBoostingClassifier\n",
      "mean accuracy score on test: 0.687398109241263\n",
      "roc_auc_score 0.49999602154049105\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# clf = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0).fit(x_train, y_train)\n",
    "# y_pred = clf.predict(x_test)\n",
    "# print('roc_auc_score', roc_auc_score(y_test, y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(max_depth=2, random_state=0, criterion = 'entropy').fit(x_train, y_train)\n",
    "# y_pred = clf.predict(x_test)\n",
    "# print('roc_auc_score', roc_auc_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "roc_auc_score 0.5144229010465293\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn import svm\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(x_train, y_train)\n",
    "# y_pred = clf.predict(x_test)\n",
    "# print('roc_auc_score', roc_auc_score(y_test, y_pred))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# def count(Data, ind, x):\n",
    "#     y = []\n",
    "#     for mcc in x:\n",
    "#         y.append((Data[Data.index == ind]['mcc_code'] == mcc).sum())\n",
    "#     return y\n",
    "\n",
    "\n",
    "# def preprocess_pipeline(drop = True, verbose = False):\n",
    "    \n",
    "#     tr_mcc_codes = pd.read_csv(\"data/tr_mcc_codes.csv\", sep=\";\", index_col=\"mcc_code\")\n",
    "#     tr_types = pd.read_csv(\"data/tr_types.csv\", sep=\";\", index_col=\"tr_type\")\n",
    "\n",
    "#     transactions = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "#     gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")\n",
    "\n",
    "#     transactions.drop(columns=['term_id'], inplace=True)\n",
    "#     transactions['tr_datetime'] = transactions['tr_datetime'].apply(lambda x: int(x.split(' ')[0]))\n",
    "#     Data = pd.merge(transactions, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "\n",
    "#     #feature aggregating\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].std(), name = 'agg_std_id', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].nunique(), name = 'agg_count_mcc_unique', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "#     Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     #keys = {}\n",
    "    \n",
    "#     # group = Data.groupby(Data['mcc_code']).apply(lambda x: x['gender'] == 1.0).groupby(level=['mcc_code']).mean()\n",
    "#     # for i in range(group.shape[0]):\n",
    "#     #     keys[group.index[i]] = group.iloc[i]\n",
    "    \n",
    "#     # agg_count_id = pd.Series(Data['mcc_code'].apply(lambda x: keys[x]), name = 'mcc_gender', index = Data.index)\n",
    "#     # Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     Data = Data[Data['gender'].notna()]\n",
    "    \n",
    "#     # columns  = [str(mcc) for mcc in Data['mcc_code'].unique()]\n",
    "#     # tempdata = pd.DataFrame(np.zeros((len(Data.index.unique()), len(Data['mcc_code'].unique()))), index=Data.index.unique(), columns=columns)\n",
    "#     # print(tempdata.shape)\n",
    "#     # indexes = Data.index.unique()\n",
    "#     # mcc_codes = Data['mcc_code'].unique()\n",
    "#     # for j, ind in enumerate(indexes):\n",
    "#     #     tempdata.apply(lambda x: count(Data, ind, mcc_codes), axis = 1)\n",
    "\n",
    "#     # Data = pd.concat([tempdata, Data], axis = 1)\n",
    "\n",
    "#     # agg_count_id = pd.Series(Data.groupby(Data['mcc_code']).  index.count(), name = 'agg_count_mcc', index = Data.index)\n",
    "#     # Data = pd.concat([agg_count_id, Data], axis = 1)\n",
    "\n",
    "#     y = Data['gender']\n",
    "#     x = Data.drop(columns = ['gender'])\n",
    "\n",
    "#     indexes = x.index   \n",
    "#     columns = x.columns\n",
    "#     x = np.nan_to_num(x)\n",
    "#     x = pd.DataFrame(StandardScaler().fit_transform(x), index=indexes, columns=columns)\n",
    "\n",
    "#     Data.index\n",
    "\n",
    "#     x_train = Data .iloc[:int(len(Data)*0.7)]\n",
    "#     x_test = Data.iloc[int(len(Data)*0.7):]\n",
    "#     x_train.index[pd.merge(x_test, x_train, left_on='customer_id', right_on='customer_id', how='inner').index].drop(inplace = True)\n",
    "    \n",
    "#     y_train = x_train['gender']\n",
    "#     x_train.drop(columns=['gender'], inplace = True)\n",
    "\n",
    "#     y_test = x_test['gender']\n",
    "#     x_test.drop(columns=['gender'], inplace = True)\n",
    "\n",
    "#     # from sklearn.model_selection import train_test_split\n",
    "#     # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y, shuffle = True)\n",
    "\n",
    "#     X_train_plus = x_train.copy()\n",
    "#     X_valid_plus = x_test.copy()\n",
    "    \n",
    "#     from sklearn.impute import SimpleImputer\n",
    "#     my_imputer = SimpleImputer()\n",
    "#     cols_with_missing = list(X_train_plus[X_train_plus.isna()].columns)\n",
    "\n",
    "#     for col in cols_with_missing:\n",
    "#         X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "#         X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "#     # Imputation\n",
    "#     my_imputer = SimpleImputer()\n",
    "#     imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "#     imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "#     # Imputation removed column names; put them back\n",
    "#     imputed_X_train_plus.columns = X_train_plus.columns\n",
    "#     imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "#     x_train = imputed_X_train_plus\n",
    "#     x_test = imputed_X_valid_plus\n",
    "\n",
    "#     if verbose:\n",
    "#         print(x_train.head())\n",
    "#     return x_train, x_test, y_train, y_test\n",
    "\n",
    "# x_train, x_test, y_train, y_test = preprocess_pipeline(verbose = True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "313b9272090493552b1bd8008684f3bd61a31a8118b7b5d481068ea70621c332"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}