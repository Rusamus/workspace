{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> Предсказание пола клиента </center>\n",
    "\n",
    "### Необходимо выявить пол клиента, основываясь на его транзакционных исторических данных. В роли метрики качества выступает [ROC AUC](https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/), который и нужно будет максимизировать."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Описание файлов\n",
    "- transactions.csv - исторические транзакции банковских клиентов\n",
    "- gender.csv - информация по полу для части клиентов (null - для тестовых)\n",
    "- tr_mcc_codes.csv - mcc-коды транзакций\n",
    "- tr_types.csv - типы транзакций\n",
    "\n",
    "## Описание полей\n",
    "### transactions.csv\n",
    "- customer_id - идентификатор клиента\n",
    "- tr_datetime - день и время совершения транзакции (дни нумеруются с начала данных)\n",
    "- mcc_code - mcc-код транзакции\n",
    "- tr_type - тип транзакции\n",
    "- amount - сумма транзакции в условных единицах; со знаком \"+\" — начисление средств клиенту, \"-\" — списание средств\n",
    "- term_id - идентификатор терминала\n",
    "\n",
    "### gender.csv\n",
    "- customer_id - идентификатор клиента\n",
    "- gender - пол клиента (пустые значения - тестовые клиенты)\n",
    "\n",
    "### tr_mcc_codes.csv\n",
    "- mcc_code - mcc-код транзакции\n",
    "- mcc_description - описание mcc-кода транзакции\n",
    "\n",
    "### tr_types.csv\n",
    "- tr_type - тип транзакции\n",
    "- tr_description - описание типа транзакции"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T13:11:26.066752Z",
     "iopub.status.busy": "2021-08-23T13:11:26.065958Z",
     "iopub.status.idle": "2021-08-23T13:11:26.073084Z",
     "shell.execute_reply": "2021-08-23T13:11:26.072336Z",
     "shell.execute_reply.started": "2021-08-23T13:11:26.066698Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задачи:\n",
    "- Разработать модель бинарной классификации для определения пола клиента. Никаких ограничений к модели - может быть что угодно от KNN до трансформеров. Главное, чтобы ROC AUC на отложенном тесте получился выше 77.5%.\n",
    "- Интерпретировать результаты модели: важность входящих в нее переменных, демонстрация на нескольких примерах, почему получился соответствующий прогноз. Последнее позволит понять, какой пол к какому из таргетов (0/1) принадлежит. Опять же, полная свобода выбора подходов! Полезные ключевые слова: gain, permutation importance, SHAP. \n",
    "- Конвертировать результаты в отчет без кода (идеально - напрямую в [html](https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html))\n",
    "\n",
    "#### P.S. Не забываем про [PEP8](https://www.python.org/dev/peps/pep-0008/)!"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#adding requirements\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tr_mcc_codes = pd.read_csv(\"data/tr_mcc_codes.csv\", sep=\";\", index_col=\"mcc_code\")\n",
    "tr_types = pd.read_csv(\"data/tr_types.csv\", sep=\";\", index_col=\"tr_type\")\n",
    "\n",
    "transactions = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:58.314241Z",
     "start_time": "2019-03-05T17:34:52.056205Z"
    },
    "execution": {
     "iopub.execute_input": "2021-08-23T13:40:21.407638Z",
     "iopub.status.busy": "2021-08-23T13:40:21.407401Z",
     "iopub.status.idle": "2021-08-23T13:40:30.132598Z",
     "shell.execute_reply": "2021-08-23T13:40:30.131875Z",
     "shell.execute_reply.started": "2021-08-23T13:40:21.407616Z"
    },
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot % missing values\n",
    "(transactions.isnull().sum(axis = 0) / transactions.shape[0] * 100).plot.bar()\n",
    "plt.title('% Missing')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('%')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check the classes distribution\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = gender['gender'].value_counts().values\n",
    "sns.barplot([0,1],x)\n",
    "plt.title('Target variable count');\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('>>>>transactions\\n' , transactions.head(10), '\\n')\n",
    "print('>>>>tr_mcc_codes\\n' , tr_mcc_codes.head(10), '\\n')\n",
    "print('>>>>tr_types\\n' , tr_types.head(10), '\\n')\n",
    "print('>>>>gender\\n' , gender.head(10), '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns; sns.set()\n",
    "corr = train.corr()\n",
    "print(corr)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.heatmap(corr);\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pca = PCA(2)\n",
    "df = pca.fit_transform(x_train)\n",
    " \n",
    "kmeans = KMeans(n_clusters= 2)\n",
    "label = kmeans.fit_predict(df)\n",
    "u_labels = np.unique(label)\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let’s start \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#adding requirements\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_feature_engineering(Data, gender):\n",
    "\n",
    "    Data = pd.merge(Data, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "    #feature aggregating\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "    Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "    Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "    Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "    Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "    Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "    Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "    Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "    keys_1 = dict(Data.groupby(Data['mcc_code'])['amount'].count())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_1[x])\n",
    "    Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "    keys_2 = dict(Data.groupby(Data['mcc_code'])['amount'].sum())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_2[x])\n",
    "    Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "    keys_3 = dict(Data.groupby(Data['mcc_code'])['amount'].nunique())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_3[x])\n",
    "    Data['agg_sum_mcc_unique_type'] = agg_count_id\n",
    "\n",
    "    mcc_amount_key = dict(Data.set_index(['mcc_code', 'gender']).groupby(level=[0,1])['amount'].mean())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: max(mcc_amount_key[(x, 1.0)] if (x, 1.0) in mcc_amount_key else 0,\n",
    "    mcc_amount_key[(x, 0.0)] if (x, 0.0) in mcc_amount_key else 0))\n",
    "    Data['mcc_amount_key'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "    Data['days_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "    Data['days_all'] = agg_count_id\n",
    " \n",
    "    keys_mcc = dict(Data.groupby(Data['mcc_code']).apply(lambda x: x['gender'] == 1.0).groupby(level=['mcc_code']).mean())\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "    Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "    keys_type = dict(Data.groupby(Data['tr_type']).apply(lambda x: x['gender'] == 1.0).groupby(level=['tr_type']).mean())\n",
    "    agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x])\n",
    "    Data['tr_type_gender'] = agg_count_id\n",
    "\n",
    "    return Data, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3 #, my_scaler\n",
    "\n",
    "def test_feature_engineering(Data, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3):\n",
    "\n",
    "    #feature aggregating\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum() > 0, name = 'agg_sign_id', index = Data.index)\n",
    "    Data['agg_sign_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].sum(), name = 'agg_sum_id', index = Data.index)\n",
    "    Data['agg_sum_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].count(), name = 'agg_count_id', index = Data.index)\n",
    "    Data['agg_count_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].mean(), name = 'agg_mean_id', index = Data.index)\n",
    "    Data['agg_mean_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].max(), name = 'agg_max_id', index = Data.index)\n",
    "    Data['agg_max_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['amount'].min(), name = 'agg_min_id', index = Data.index)\n",
    "    Data['agg_min_id'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['mcc_code'].count(), name = 'agg_count_mcc', index = Data.index)\n",
    "    Data['agg_count_mcc'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_1[x])\n",
    "    Data['agg_count_mcc_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_2[x])\n",
    "    Data['agg_sum_mcc'] = agg_count_id\n",
    "    \n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_3[x])\n",
    "    Data['agg_sum_mcc_unique_type'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: max(mcc_amount_key[(x, 1.0)] if (x, 1.0) in mcc_amount_key else 0,\n",
    "     mcc_amount_key[(x, 0.0)] if (x, 0.0) in mcc_amount_key else 0))\n",
    "    Data['mcc_amount_key'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].nunique(), name = 'days_unique', index = Data.index)\n",
    "    Data['days_unique'] = agg_count_id\n",
    "\n",
    "    agg_count_id = pd.Series(Data.groupby(Data.index)['tr_datetime'].count(), name = 'days_all', index = Data.index)\n",
    "    Data['days_all'] = agg_count_id\n",
    " \n",
    "    agg_count_id = Data['mcc_code'].apply(lambda x: keys_mcc[x])\n",
    "    Data['mcc_gender'] = agg_count_id\n",
    "\n",
    "    agg_count_id = Data['tr_type'].apply(lambda x: keys_type[x] if x in keys_type else 0)\n",
    "    Data['tr_type_gender'] = agg_count_id\n",
    "    \n",
    "    return Data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def preprocess_pipeline(drop = True, verbose = False):\n",
    "\n",
    "    SPLIT_RATIO = 0.99\n",
    "    \n",
    "    tr_mcc_codes = pd.read_csv(\"data/tr_mcc_codes.csv\", sep=\";\", index_col=\"mcc_code\")\n",
    "    tr_types = pd.read_csv(\"data/tr_types.csv\", sep=\";\", index_col=\"tr_type\")\n",
    "\n",
    "    transactions = pd.read_csv(\"data/transactions.csv\", index_col=\"customer_id\")\n",
    "    gender = pd.read_csv(\"data/gender.csv\", index_col=\"customer_id\")\n",
    "\n",
    "    transactions.drop(columns=['term_id'], inplace=True)\n",
    "    transactions['tr_datetime'] = transactions['tr_datetime'].apply(lambda x: int(x.split(' ')[0]))\n",
    "\n",
    "    ind = transactions.index.unique()[int(len(transactions.index.unique())*SPLIT_RATIO)]\n",
    "    train = transactions[transactions.index >= ind]\n",
    "    test = transactions[transactions.index < ind]\n",
    "\n",
    "    print('fraction of unique people in test', (len(test.index.unique())/(len(test.index.unique()) + len(train.index.unique())))*100)\n",
    "    print('fraction of test set', (len(test.index)/(len(test.index) + len(train.index)))*100)\n",
    "\n",
    "    train, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3 = train_feature_engineering(train, gender)\n",
    "    test = test_feature_engineering(test, keys_mcc, keys_type, mcc_amount_key, keys_1, keys_2, keys_3)\n",
    "    test = pd.merge(test, gender, left_on='customer_id', right_on='customer_id', how='inner')\n",
    "    \n",
    "    train_nan = train[train['gender'].isna()]\n",
    "    test_nan = test[test['gender'].isna()]\n",
    "    data_nan = pd.concat([train_nan, test_nan], axis = 0)\n",
    "    data_nan.drop(columns=['gender'], inplace=True)\n",
    "    \n",
    "    train = train[train['gender'].notna()]\n",
    "    test = test[test['gender'].notna()]\n",
    "\n",
    "    return train, test, data_nan\n",
    "\n",
    "train, test, data_nan = preprocess_pipeline(verbose = True)\n",
    "print('finish train.shape, test.shape', train.shape, test.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train = train['gender']\n",
    "x_train = train.drop(columns = ['gender'])\n",
    "y_test = test['gender']\n",
    "x_test = test.drop(columns = ['gender'])\n",
    "\n",
    "scale =  False\n",
    "if scale:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "print('>>>>>>>>>', 'XGBClassifier')\n",
    "\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=5, monotone_constraints='()',\n",
    "              n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "              silent=True, subsample=0.8, tree_method='exact',\n",
    "              validate_parameters=1, verbosity=None)\n",
    "              \n",
    "model.fit(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#add_nans\n",
    "nans = True\n",
    "if nans:\n",
    "    y_pred_nan = pd.DataFrame(model.predict(data_nan), index = data_nan.index)\n",
    "    X_train = pd.concat([x_train, data_nan], axis = 0)\n",
    "    Y_train = pd.concat([y_train, y_pred_nan], axis = 0)\n",
    "    print(X_train.shape, x_train.shape)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "y_pred = model.predict(x_train)\n",
    "roc_auc_train = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "print('mean accuracy score on test: ' + str(accuracy))\n",
    "print('roc_auc_score on test: '+ str(roc_auc_test))\n",
    "print('roc_auc_score on train: '+ str(roc_auc_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [5, 6, 7, 8, 9]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "\n",
    "folds = 5\n",
    "param_comb = 4\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(x_train,y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) \n",
    "random_search.fit(x_train, y_train)\n",
    "timer(start_time) \n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#add_nans\n",
    "nans = False\n",
    "if nans:\n",
    "    y_pred_nan = pd.DataFrame(model.predict(data_nan), index = data_nan.index)\n",
    "    x_train = pd.concat([x_train, data_nan], axis = 0)\n",
    "    y_train = pd.concat([y_train, y_pred_nan], axis = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SHAP/ feature importance\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shap\n",
    "\n",
    "shap_test = shap.TreeExplainer(model).shap_values(x_train)\n",
    "shap.summary_plot(shap_test, x_train,\n",
    "                      max_display=25, auto_size_plot=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "313b9272090493552b1bd8008684f3bd61a31a8118b7b5d481068ea70621c332"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}